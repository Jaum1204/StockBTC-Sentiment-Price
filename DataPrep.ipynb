{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import nltk\n",
    "import ssl\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import en_core_web_md\n",
    "text_to_nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "invisible_chars_pattern = re.compile(\n",
    "    '[\\u200B\\u200C\\u200D\\u200E\\u200F\\uFEFF\\u00A0]'\n",
    ")\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = pd.read_csv(\"./Raw/SPX.csv\")\n",
    "df_BP = pd.read_csv(\"./Raw/BTC-USD.csv\")\n",
    "\n",
    "df_SP['Date'] = pd.to_datetime(df_SP['Date'])\n",
    "df_BP['Date'] = pd.to_datetime(df_BP['Date'])\n",
    "\n",
    "df_SP = df_SP.rename(columns={\n",
    "    'Open': 'Open_S',\n",
    "    'High': 'High_S',\n",
    "    'Low': 'Low_S',\n",
    "    'Close': 'Close_S',\n",
    "    'Adj Close': 'Adj Close_S',\n",
    "    'Volume': 'Volume_S'\n",
    "})\n",
    "\n",
    "df_BP = df_BP.rename(columns={\n",
    "    'Open': 'Open_B',\n",
    "    'High': 'High_B',\n",
    "    'Low': 'Low_B',\n",
    "    'Close': 'Close_B',\n",
    "    'Adj Close': 'Adj Close_B',\n",
    "    'Volume': 'Volume_B'\n",
    "})\n",
    "\n",
    "merged_prices = pd.merge(df_SP, df_BP, on='Date')\n",
    "\n",
    "file_path = os.path.join('./Processed', 'combined_prices.csv')\n",
    "\n",
    "merged_prices.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = invisible_chars_pattern.sub('', text)\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    clean_tokens = []\n",
    "    for token in text_to_nlp(text):\n",
    "        if (not token.is_stop) and (token.lemma_ != '-PRON-') and (not token.is_punct):\n",
    "            clean_tokens.append(token.lemma_)\n",
    "    return clean_tokens\n",
    "\n",
    "def process_lang_data(text):\n",
    "  cleaned_text = []\n",
    "  punctuation = string.punctuation\n",
    "  our_stopwords = stopwords.words('english')\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  for token in word_tokenize(text):\n",
    "    if token not in punctuation and token not in our_stopwords:\n",
    "      clipped_token = lemmatizer.lemmatize(token)\n",
    "      cleaned_text.append(clipped_token)\n",
    "\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                Date                                               body  \\\n",
       "0        2014-09-18  #RDD / #BTC on the exchanges:\\nCryptsy: 0.0000...   \n",
       "1        2014-09-18  Current price: 418.77$ $BTCUSD $btc #bitcoin 2...   \n",
       "2        2014-09-18  1 #BTC (#Bitcoin) quotes:\\n$423.60/$424.80 #Bi...   \n",
       "3        2014-09-18  In the last 10 mins, there were arb opps spann...   \n",
       "4        2014-09-18  Be judicious, buy your Bitcoins at https://Bit...   \n",
       "...             ...                                                ...   \n",
       "18452496 2019-11-23  €400 million investment in Blockchain and AI t...   \n",
       "18452497 2019-11-23  BTC/USD | $BTCUSD | $BTC $USD\\n\\nBitcoin Outlo...   \n",
       "18452498 2019-11-23  BTC\\n\\n長期的目線\\n\\n現在のトライアングル収束までに要した期間と人々の関心から、\\...   \n",
       "18452499 2019-11-23  SPECIAL DEAL TO ANYONE HAS CASH APP OR BITCOIN...   \n",
       "18452500 2019-11-23  $BTC - an update on the longer term view for B...   \n",
       "\n",
       "          Sentiment  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "18452496          1  \n",
       "18452497          1  \n",
       "18452498          1  \n",
       "18452499          1  \n",
       "18452500          1  \n",
       "\n",
       "[18452501 rows x 3 columns]>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BS = pd.read_csv(\"./Raw/BitcoinSent.csv\")\n",
    "\n",
    "#clean dataframe\n",
    "df_BS['Date'] = pd.to_datetime(df_BS['Date'], errors='coerce')\n",
    "df_BS = df_BS[df_BS['Date'].notna()]\n",
    "df_BS = df_BS[df_BS['text'].notna()]\n",
    "df_BS = df_BS[df_BS['text'].apply(lambda x: isinstance(x, str))]\n",
    "df_BS = df_BS[df_BS['Sentiment'].notna()]\n",
    "df_BS = df_BS[df_BS['Sentiment'].isin(['Positive', 'Negative'])]\n",
    "df_BS['Sentiment'] = df_BS['Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "df_BS.rename(columns={'text': 'body'}, inplace=True)\n",
    "\n",
    "#sort in ascending date\n",
    "df_BS = df_BS.sort_values(by='Date', ascending=True)\n",
    "df_BS = df_BS.reset_index(drop=True)\n",
    "\n",
    "#testing\n",
    "df_BS.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of            Date                                               body  Sentiment\n",
       "0    2014-09-18  [current, price, 447.92, $, $, BTCUSD, $, btc,...          0\n",
       "1    2014-09-18  [Bitcoin, trade, @bleutrade, $, 450.00, @btcec...          0\n",
       "2    2014-09-18  [2014年9月18日, 18:00:09, btc_jpy, 直近[last]:49720...          0\n",
       "3    2014-09-18  [rdd, BTC, exchange, Cryptsy, 0.00000014, Bitt...          0\n",
       "4    2014-09-19  [current, price, 307.21, €, $, BTCEUR, $, btc,...          0\n",
       "...         ...                                                ...        ...\n",
       "7563 2019-11-22  [BTC, 6000$〜7000, $, BTCにおいて、6000$、7000$は重要な意味...          0\n",
       "7564 2019-11-23  [@extstock, want, receive, free, 100, BTB, cry...          1\n",
       "7565 2019-11-23  [Bitcoin, Dives, month, Low, China, Crackdown,...          1\n",
       "7566 2019-11-23  [discover, easily, maximise, trade, enter, 📈, ...          1\n",
       "7567 2019-11-23  [whale, dump, $, btc, need, buy, hope, btc, 3,...          1\n",
       "\n",
       "[7568 rows x 3 columns]>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample dataframe\n",
    "sampled_BS = df_BS.groupby('Date').apply(lambda x: x.sample(n=min(len(x), 4))).reset_index(drop=True)\n",
    "\n",
    "#clean every text\n",
    "sampled_BS['body'] = sampled_BS['body'].apply(clean_text)\n",
    "\n",
    "#apply tokenizer\n",
    "sampled_BS['body'] = sampled_BS['body'].apply(tokenize)\n",
    "\n",
    "'''\n",
    "print(sampled_BS.tail)\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "sampled_BS['language'] = sampled_BS['body'].apply(detect_language)\n",
    "sampled_BS = sampled_BS[sampled_BS['language'] == 'en'].copy()\n",
    "sampled_BS.drop(columns=['language'], inplace=True)\n",
    "sampled_BS = sampled_BS.reset_index(drop=True)\n",
    "\n",
    "print(sampled_BS.tail)\n",
    "'''\n",
    "\n",
    "sampled_BS.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join dates and merge text\n",
    "test = sampled_BS\n",
    "test['body_str'] = test['body'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "test = test.groupby('Date').agg({\n",
    "    'body': list,\n",
    "    'body_str': lambda texts: ' '.join(texts),\n",
    "    'Sentiment': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "file_path = os.path.join('./Processed', 'btc_nlp_test.csv')\n",
    "test.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                Date                                               body\n",
       "0        2015-01-01  lx21 made $10,008  on $AAPL -Check it out! htt...\n",
       "1        2015-01-01  Insanity of today weirdo massive selling. $aap...\n",
       "2        2015-01-01  S&P100 #Stocks Performance $HD $LOW $SBUX $TGT...\n",
       "3        2015-01-01  $GM $TSLA: Volkswagen Pushes 2014 Record Recal...\n",
       "4        2015-01-01  Swing Trading: Up To 8.91% Return In 14 Days h...\n",
       "...             ...                                                ...\n",
       "3717959  2019-12-31  That $SPY $SPX puuump in the last hour was the...\n",
       "3717960  2019-12-31  In 2020 I may start Tweeting out positive news...\n",
       "3717961  2019-12-31  Patiently Waiting for the no twitter sitter tw...\n",
       "3717962  2019-12-31  I don't discriminate. I own both $aapl and $ms...\n",
       "3717963  2019-12-31  $AAPL #patent 10,522,475 Vertical interconnect...\n",
       "\n",
       "[3717964 rows x 2 columns]>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SS = pd.read_csv(\"./Raw/StockSent.csv\")\n",
    "\n",
    "df_SS['post_date'] = pd.to_datetime(df_SS['post_date'], unit='s')\n",
    "df_SS['post_date'] = df_SS['post_date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df_SS.rename(columns={'post_date': 'Date'}, inplace=True)\n",
    "df_SS.drop(columns=['tweet_id', 'writer', 'comment_num', 'retweet_num', 'like_num'], inplace=True)\n",
    "\n",
    "df_SS.tail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
