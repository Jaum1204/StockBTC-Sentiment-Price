{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import nltk\n",
    "import ssl\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "import en_core_web_md\n",
    "text_to_nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "INVISIBLE_CHARS = re.compile(r'[\\u200B\\u200C\\u200D\\u200E\\u200F\\uFEFF\\u00A0]')\n",
    "URLS = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "EMOJIS = re.compile(\"[\"\n",
    "                    u\"\\U0001F600-\\U0001F64F\"\n",
    "                    u\"\\U0001F300-\\U0001F5FF\"\n",
    "                    u\"\\U0001F680-\\U0001F6FF\"\n",
    "                    u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                    u\"\\U00002700-\\U000027BF\"\n",
    "                    u\"\\U000024C2-\\U0001F251\"\n",
    "                    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "REMOVE_PUNCT_DIGITS = str.maketrans('', '', string.punctuation + string.digits)\n",
    "CUSTOM_STOPWORDS = {\n",
    "    '|', '=', '1', '5', '2018', 'usd', 'price', 'exchange',\n",
    "    '€', '24', 'utc', 'en', 'high', 'low', 'volume'\n",
    "}\n",
    "BITCOIN_ALIASES = {'btc', 'bit', 'bitcoin', 'bitcoins', 'bit coin'}\n",
    "OTHER_CRYPTO = {\n",
    "    'eth', 'ethereum', 'xrp', 'bch', 'ltc', 'etc', 'ada', 'doge', 'shiba',\n",
    "    'polkadot', 'dot', 'bnb', 'solana', 'trx', 'eos', 'neo', 'iota', 'monero',\n",
    "    'dash', 'zec', 'vechain', 'theta', 'stellar', 'xlm', 'avax', 'algo',\n",
    "    'matic', 'near', 'icp', 'aptos', 'apt', 'kaspa', 'kas'\n",
    "}\n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP = pd.read_csv(\"./Raw/SPX.csv\")\n",
    "df_BP = pd.read_csv(\"./Raw/BTC-USD.csv\")\n",
    "\n",
    "df_SP['Date'] = pd.to_datetime(df_SP['Date'])\n",
    "df_BP['Date'] = pd.to_datetime(df_BP['Date'])\n",
    "\n",
    "df_SP = df_SP.rename(columns={\n",
    "    'Open': 'Open_S',\n",
    "    'High': 'High_S',\n",
    "    'Low': 'Low_S',\n",
    "    'Close': 'Close_S',\n",
    "    'Adj Close': 'Adj Close_S',\n",
    "    'Volume': 'Volume_S'\n",
    "})\n",
    "\n",
    "df_BP = df_BP.rename(columns={\n",
    "    'Open': 'Open_B',\n",
    "    'High': 'High_B',\n",
    "    'Low': 'Low_B',\n",
    "    'Close': 'Close_B',\n",
    "    'Adj Close': 'Adj Close_B',\n",
    "    'Volume': 'Volume_B'\n",
    "})\n",
    "\n",
    "merged_prices = pd.merge(df_SP, df_BP, on='Date')\n",
    "\n",
    "file_path = os.path.join('./Processed', 'combined_prices.csv')\n",
    "\n",
    "merged_prices.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return ''\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = INVISIBLE_CHARS.sub('', text)\n",
    "    text = URLS.sub('', text)\n",
    "    text = EMOJIS.sub('', text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(REMOVE_PUNCT_DIGITS)\n",
    "    tokens = text.split()\n",
    "\n",
    "    has_bitcoin = False\n",
    "    clean_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in OTHER_CRYPTO or token in CUSTOM_STOPWORDS:\n",
    "            continue\n",
    "        \n",
    "      \n",
    "        if token in BITCOIN_ALIASES:\n",
    "            if has_bitcoin:\n",
    "                continue \n",
    "            else:\n",
    "                clean_tokens.append('bitcoin')\n",
    "                has_bitcoin = True\n",
    "        else:\n",
    "            clean_tokens.append(token)\n",
    "\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "def tokenize(text):\n",
    "    clean_tokens = []\n",
    "    for token in text_to_nlp(text):\n",
    "        if (not token.is_stop) and (token.lemma_ != '-PRON-') and (not token.is_punct):\n",
    "            clean_tokens.append(token.lemma_)\n",
    "    return clean_tokens\n",
    "\n",
    "def process_lang_data(text):\n",
    "  cleaned_text = []\n",
    "  punctuation = string.punctuation\n",
    "  our_stopwords = stopwords.words('english')\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  for token in word_tokenize(text):\n",
    "    if token not in punctuation and token not in our_stopwords:\n",
    "      clipped_token = lemmatizer.lemmatize(token)\n",
    "      cleaned_text.append(clipped_token)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                Date                                               body  \\\n",
       "0        2014-09-18  #RDD / #BTC on the exchanges:\\nCryptsy: 0.0000...   \n",
       "1        2014-09-18  Current price: 418.77$ $BTCUSD $btc #bitcoin 2...   \n",
       "2        2014-09-18  1 #BTC (#Bitcoin) quotes:\\n$423.60/$424.80 #Bi...   \n",
       "3        2014-09-18  In the last 10 mins, there were arb opps spann...   \n",
       "4        2014-09-18  Be judicious, buy your Bitcoins at https://Bit...   \n",
       "...             ...                                                ...   \n",
       "18452496 2019-11-23  €400 million investment in Blockchain and AI t...   \n",
       "18452497 2019-11-23  BTC/USD | $BTCUSD | $BTC $USD\\n\\nBitcoin Outlo...   \n",
       "18452498 2019-11-23  BTC\\n\\n長期的目線\\n\\n現在のトライアングル収束までに要した期間と人々の関心から、\\...   \n",
       "18452499 2019-11-23  SPECIAL DEAL TO ANYONE HAS CASH APP OR BITCOIN...   \n",
       "18452500 2019-11-23  $BTC - an update on the longer term view for B...   \n",
       "\n",
       "          Sentiment  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "18452496          1  \n",
       "18452497          1  \n",
       "18452498          1  \n",
       "18452499          1  \n",
       "18452500          1  \n",
       "\n",
       "[18452501 rows x 3 columns]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BS = pd.read_csv(\"./Raw/BitcoinSent.csv\")\n",
    "\n",
    "#clean dataframe\n",
    "df_BS['Date'] = pd.to_datetime(df_BS['Date'], errors='coerce')\n",
    "df_BS = df_BS[df_BS['Date'].notna()]\n",
    "df_BS = df_BS[df_BS['text'].notna()]\n",
    "df_BS = df_BS[df_BS['text'].apply(lambda x: isinstance(x, str))]\n",
    "df_BS = df_BS[df_BS['Sentiment'].notna()]\n",
    "df_BS = df_BS[df_BS['Sentiment'].isin(['Positive', 'Negative'])]\n",
    "df_BS['Sentiment'] = df_BS['Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
    "\n",
    "df_BS.rename(columns={'text': 'body'}, inplace=True)\n",
    "\n",
    "#sort in ascending date\n",
    "df_BS = df_BS.sort_values(by='Date', ascending=True)\n",
    "df_BS = df_BS.reset_index(drop=True)\n",
    "\n",
    "#testing\n",
    "df_BS.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR VISUALIZATION - OTHER FILE\n",
    "mask = (df_BS['Date'] >= '2018-01-07') & (df_BS['Date'] <= '2018-02-06')\n",
    "\n",
    "visu_words = df_BS.loc[mask]\n",
    "visu_words.drop(columns='Sentiment', inplace=True)\n",
    "visu_words = visu_words.reset_index(drop=True)\n",
    "\n",
    "visu_words = visu_words.groupby('Date').apply(lambda x: x.sample(n=min(len(x), 800))).reset_index(drop=True)\n",
    "\n",
    "visu_words['body'] = visu_words['body'].apply(clean_text)\n",
    "visu_words['body'] = visu_words['body'].apply(tokenize)\n",
    "\n",
    "visu_words.tail\n",
    "visu_words.to_pickle('visu_words.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of            Date                                               body  Sentiment\n",
       "0    2014-09-18  [10, min, arb, opp, span, 36, exchange, pair, ...          0\n",
       "1    2014-09-18  [live, profit, 30950, 314, buy, b2271, 43189, ...          0\n",
       "2    2014-09-18  [45099, 0500, utc, 24h, range, 44357, 46284, v...          0\n",
       "3    2014-09-18  [bitcoineurrate, €, 329, €, 3327, buy, €, 329,...          0\n",
       "4    2014-09-18  [10, min, arb, opp, span, 39, exchange, pair, ...          0\n",
       "...         ...                                                ...        ...\n",
       "9455 2019-11-23                        [mnsairdrop, monnosairdrop]          1\n",
       "9456 2019-11-23  [bitcoin, whale, move, 47000, bitcoin, 338, mi...          1\n",
       "9457 2019-11-23  [price, 1, ltc, usd, 00, change, 1000, price, ...          1\n",
       "9458 2019-11-23  [think, agree, vitaliks, comment, mountain, ma...          1\n",
       "9459 2019-11-23  [15, m, volume, alert, phb, current, volume, 2...          1\n",
       "\n",
       "[9460 rows x 3 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample dataframe\n",
    "sampled_BS = df_BS.groupby('Date').apply(lambda x: x.sample(n=min(len(x), 5))).reset_index(drop=True)\n",
    "\n",
    "#clean every text\n",
    "sampled_BS['body'] = sampled_BS['body'].apply(clean_text)\n",
    "\n",
    "#apply tokenizer\n",
    "sampled_BS['body'] = sampled_BS['body'].apply(tokenize)\n",
    "\n",
    "'''\n",
    "print(sampled_BS.tail)\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "sampled_BS['language'] = sampled_BS['body'].apply(detect_language)\n",
    "sampled_BS = sampled_BS[sampled_BS['language'] == 'en'].copy()\n",
    "sampled_BS.drop(columns=['language'], inplace=True)\n",
    "sampled_BS = sampled_BS.reset_index(drop=True)\n",
    "\n",
    "print(sampled_BS.tail)\n",
    "'''\n",
    "\n",
    "sampled_BS.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join dates and merge text\n",
    "test = sampled_BS\n",
    "test['body_str'] = test['body'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "test = test.groupby('Date').agg({\n",
    "    'body': list,\n",
    "    'body_str': lambda texts: ' '.join(texts),\n",
    "    'Sentiment': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "file_path = os.path.join('./Processed', 'btc_nlp_test.csv')\n",
    "test.to_csv(file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                Date                                               body\n",
       "0        2015-01-01  lx21 made $10,008  on $AAPL -Check it out! htt...\n",
       "1        2015-01-01  Insanity of today weirdo massive selling. $aap...\n",
       "2        2015-01-01  S&P100 #Stocks Performance $HD $LOW $SBUX $TGT...\n",
       "3        2015-01-01  $GM $TSLA: Volkswagen Pushes 2014 Record Recal...\n",
       "4        2015-01-01  Swing Trading: Up To 8.91% Return In 14 Days h...\n",
       "...             ...                                                ...\n",
       "3717959  2019-12-31  That $SPY $SPX puuump in the last hour was the...\n",
       "3717960  2019-12-31  In 2020 I may start Tweeting out positive news...\n",
       "3717961  2019-12-31  Patiently Waiting for the no twitter sitter tw...\n",
       "3717962  2019-12-31  I don't discriminate. I own both $aapl and $ms...\n",
       "3717963  2019-12-31  $AAPL #patent 10,522,475 Vertical interconnect...\n",
       "\n",
       "[3717964 rows x 2 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SS = pd.read_csv(\"./Raw/StockSent.csv\")\n",
    "\n",
    "df_SS['post_date'] = pd.to_datetime(df_SS['post_date'], unit='s')\n",
    "df_SS['post_date'] = df_SS['post_date'].dt.strftime('%Y-%m-%d')\n",
    "df_SS.rename(columns={'post_date': 'Date'}, inplace=True)\n",
    "\n",
    "df_SS.drop(columns=['tweet_id', 'writer', 'comment_num', 'retweet_num', 'like_num'], inplace=True)\n",
    "\n",
    "df_SS.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of             Date                                               body\n",
       "0     2015-01-01  [memo, elonmusk, not, chestnut, roast, open, f...\n",
       "1     2015-01-01  [timcook, hope, resolution, increase, buyback,...\n",
       "2     2015-01-01  [close, year, 176, big, gainer, yhoo, peix, kn...\n",
       "3     2015-01-01  [jeff, bezos, lose, 74, billion, amazon, bad, ...\n",
       "4     2015-01-01              [myth, risk, option, cmg, tsla, bidu]\n",
       "...          ...                                                ...\n",
       "9125  2019-12-31  [fwiw, high, performance, vehicle, sua, high, ...\n",
       "9126  2019-12-31  [good, 6, month, s, 1st, 6, month, awfulan, in...\n",
       "9127  2019-12-31        [warn, not, believe, come, fwed, mouthtsla]\n",
       "9128  2019-12-31  [ron, baron, legendary, investor, 30b, managem...\n",
       "9129  2019-12-31  [nio, tsla, snap, spce, jnug, acb, attach, pos...\n",
       "\n",
       "[9130 rows x 2 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample dataframe\n",
    "sampled_SS = df_SS.groupby('Date').apply(lambda x: x.sample(n=min(len(x), 5))).reset_index(drop=True)\n",
    "\n",
    "#clean every text\n",
    "sampled_SS['body'] = sampled_SS['body'].apply(clean_text)\n",
    "\n",
    "#apply tokenizer\n",
    "sampled_SS['body'] = sampled_SS['body'].apply(tokenize)\n",
    "\n",
    "sampled_SS.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join dates and merge text\n",
    "testS = sampled_SS\n",
    "testS['body_str'] = testS['body'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "testS = testS.groupby('Date').agg({\n",
    "    'body': list,\n",
    "    'body_str': lambda texts: ' '.join(texts),\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "file_path = os.path.join('./Processed', 'stock_nlp_test.csv')\n",
    "testS.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
